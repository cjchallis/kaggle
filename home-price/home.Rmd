---
output:
  knitrBootstrap::bootstrap_document:
    title: "Home Prices"
    theme: amelia
    highlight: sunburst
    theme.chooser: TRUE
    highlight.chooser: TRUE
---
## Data Driven Real Estate Analysis

This originated as a Kaggle kernel from Stephanie Kirmer. I will be starting with her script, but attempting to go through the analysis myself.

```{r Libraries, message=FALSE, warning=FALSE, echo=FALSE}
#library(data.table)
#library(FeatureHashing)
#library(Matrix)
#library(xgboost)
#library(randomForest)
library(caret)
library(dplyr)
#library(ggplot2)
#library(pROC)
#library(stringr)
#library(dummies)
library(Metrics)
#library(kernlab)
#library(mlbench)
```

### Data Import

I almost always use `stringsAsFactors=FALSE`, but in this case there are many string-valued characters I would like to treat as factors. 
```{r load, echo=FALSE}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
names(train)
```

One thing I try to look for first is missing values in the data. There are methods that can ignore or replace missing values, but I like to understand why they are present and determine if there are any natural ways to handle these values.
```{r find_na}
nas = apply(is.na(train), 2, sum)
nas[nas > 0]
```

There are a few different categories of missing values here:
* Factors, in which `NA` can simply be another level of the factor. In the metadata many of the fields explicitly state that `NA` means 'None,' so it is most appropriate to include this as separate category.
* Continuous attributes of a feature which not all homes have, such as a wood deck. In `WoodDeckSF` the square footage of the deck is given where a deck is present, the value is `NA` otherwise. I prefer to split this into two variables: a binary variable that indicates the presence of a wood deck, and a centered square footage variable. This separates the information into two features that models can evaluate - whether a wood deck exists, and how the deck compares to an average deck. This is especially helpful to linear models, but is also useful in interpreting the variables selected by any algorithm.
* Groups of variables related to a feature which not all homes have. Here I create one indicator for the group, then treat the individual variables as described in the previous two bullets. In this dataset there are seven garage variables.


I also want to check for zero values, as it looks like sometimes `0` is used when a variable does not apply.

```{r find_na}
zeroes = apply(!is.na(train) & train == 0, 2, sum)
zeroes[zeroes > 0]
```

I originally wanted to collapse the 4 porch variables into two: a factor with the type of porch and a square footage variable, but it turns out that the types of porches are not mutually exclusive in the dataset.

```{r handle_na}
factors = c("Alley", "Fence", "Electrical")
porches = c("OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch")
notAlwaysApplicable = c("WoodDeckSF", "LotFrontage", "X2ndFlrSF", porches)
varGroups = c("Bsmt", "Garage", "Fireplace", "Pool", "MasVnr", "Misc")
repVars = c("BsmtQual", "GarageQual", "FireplaceQu", "PoolQC", "MasVnrType", "MiscFeature")
names(repVars) = varGroups


type1_unf = !is.na(train$BsmtFinType1) & train$BsmtFinType1 == "Unf"
train$BsmtFinSF1[type1_unf] = train$BsmtUnfSF[type1_unf]
train$BsmtUnfSF[type1_unf] = 0

type2_unf = train$BsmtFinType1 != "Unf" & !is.na(train$BsmtFinType2) & train$BsmtFinType2 == "Unf"
train$BsmtFinSF2[type2_unf] = train$BsmtUnfSF[type2_unf]
train$BsmtUnfSF[type2_unf] = 0


for (f in factors){
  temp = as.character(train[[f]])
  temp[is.na(temp)] = "NA"
  train[[f]] = factor(temp)
}

for (var in notAlwaysApplicable){
  applicable = !is.na(train[[var]]) & train[[var]] > 0
  train[[var]] = train[[var]] - mean(train[[var]][applicable])
  train[[var]][!applicable] = 0
  indicator = sprintf("%sInd", var)
  train[[indicator]] = as.numeric(applicable)
}

for (group in varGroups){
  vars = grep(group, names(train), value=TRUE)
  rep = repVars[group]
  indicator = sprintf("%sInd", group)
  applicable = !is.na(train[[rep]])
  train[[indicator]] = as.numeric(applicable)
  for (v in vars){
    if (is.factor(train[[v]])){
      #temp = as.character(train[[v]])
      #temp[is.na(temp)] = "NA"
      #train[[v]] = factor(temp)
    } else {
      m = mean(train[[v]][applicable])
      train[[v]] = train[[v]] - m
      train[[v]][!applicable] = 0
    }
  }
}

nas = apply(is.na(train), 2, sum)
nas[nas > 0]

zeroes = apply(!is.na(train) & train == 0, 2, sum)
zeroes[zeroes > 0]
```



numericToFactor = c("MSSubClass", "MoSold")
There are a few problem variables we need to take care of. First, `MSSubClass` should be a factor, but its levels are given as integers so it is read as a numeric variable. It is probably also more appropriate to treat `MoSold` (month sold) as a factor.
```{r MSSubClass}
train$MSSubClass = as.factor(train$MSSubClass)
test$MSSubClass = as.factor(test$MSSubClass)
train$MoSold = as.factor(train$MoSold)
test$MoSold = as.factor(test$MoSold)

```


Let's take a look at the marginal distribution of sale price.

```{r histogram}
plot(hist(train$SalePrice))
```

Unsurprisingly, the sale price distribution looks log-normal. We also have the benefit of knowing that the competition is judged on root mean squared logarithmic error, so let's take the log and work with that.

```{r log_sale}
train$LogSalePrice = log(train$SalePrice)
plot(hist(train$LogSalePrice))
```

### Train/Test Partitions

This uses the `caret` package.

```{r partition}
partition <- createDataPartition(y=train$LogSalePrice,
                                 p=.7,
                                 list=F)
training <- train[partition,]
testing <- train[-partition,]
```

### Linear Model

A linear model can be a good part of data exploration. It is possible in generating the train/test partitions that some variables are now singular in the training data.

```{r lm}
singular = NULL
for (var in names(training)){
  freq = table(training[var])
  if (length(freq[freq > 0]) < 2)
    singular = c(singular, var)
}
no_train = NULL
for (var in names(training)){
  if (is.factor(training[[var]])){
    tr = table(training[[var]])
    te = table(testing[[var]])
    if (any(tr == 0 & te > 0))
      no_train = c(no_train, var)
  }
}
print(paste("Singular variables in the training partition:", paste(singular, collapse = " ")))
print(paste("Factor variables with levels in test partition but not training:", paste(no_train, collapse = " ")))
exclude = c(singular, no_train, "PoolQC", "MiscFeature")
training_exclude = training
if (length(exclude > 0))
  training_exclude = training %>% select(-one_of(exclude))
lm_model <- lm(LogSalePrice ~ . - SalePrice, data=training_exclude)
summary(lm_model)

# for (i in 1:length(names(training_exclude))){
#   fart = data.frame(training_exclude[,1:i])
#   fart$LogSalePrice = training_exclude$LogSalePrice
#   lm(LogSalePrice ~ ., data=fart)
# }
```

```{r lm_rmse}
prediction <- predict(lm_model, testing, type="response")

rmse(testing$logSalePrice, prediction)
```

```{r var_sel}
coef_table = summary(lm_model)$coefficients
SIG_T = 3.5
significant = summ[,1][abs(summ[,3]) > SIG_T]
var_names = names(significant)
print(var_names)

vars = NULL
for (var in names(train)){
  if (length(grep(var, var_names)) > 0)
    vars = c(vars, var)
}
```
ALL STEPHANIE BELOW THIS POINT

Lots of stuff we can drop right off, that's good. Some multicollinearity is making the model drop a few variables, but that's ok.

Also, our R-squared is not too bad! In case you're unfamiliar, that indicates what percent of the variation in the outcome is explained using the model we designed.

```{r lm2}

lm_model_15 <- lm(SalePrice ~ MSSubClass+LotArea+BsmtUnfSF+
                    X1stFlrSF+X2ndFlrSF+GarageCars+
                    WoodDeckSF+nbhd_price_level+
                    exterior_cond+pos_features_1+
                    bsmt_exp+kitchen+housefunction+pool_good+sale_cond+
                    qual_ext+qual_bsmt, data=training)
summary(lm_model_15)

```

That's our model with the important stuff, more or less. How does the RMSE turn out? That is our outcome of interest, after all.


```{r testing}

prediction <- predict(lm_model_15, testing, type="response")
model_output <- cbind(testing, prediction)

model_output$log_prediction <- log(model_output$prediction)
model_output$log_SalePrice <- log(model_output$SalePrice)

#Test with RMSE

rmse(model_output$log_SalePrice,model_output$log_prediction)

```

###A Random Forest

Not too bad, given that this is just an LM. Let's try training the model with an RF. Let's use all the variables and see how things look, since randomforest does its own feature selection.

```{r caret1}

model_1 <- randomForest(SalePrice ~ ., data=training)


# Predict using the test set
prediction <- predict(model_1, testing)
model_output <- cbind(testing, prediction)


model_output$log_prediction <- log(model_output$prediction)
model_output$log_SalePrice <- log(model_output$SalePrice)

#Test with RMSE

rmse(model_output$log_SalePrice,model_output$log_prediction)


```

###An xgboost
Nice! Try it with xgboost?

```{r matrices}

#Assemble and format the data

training$log_SalePrice <- log(training$SalePrice)
testing$log_SalePrice <- log(testing$SalePrice)

#Create matrices from the data frames
trainData<- as.matrix(training, rownames.force=NA)
testData<- as.matrix(testing, rownames.force=NA)

#Turn the matrices into sparse matrices
train2 <- as(trainData, "sparseMatrix")
test2 <- as(testData, "sparseMatrix")

#####
#colnames(train2)
#Cross Validate the model

vars <- c(2:37, 39:86) #choose the columns we want to use in the prediction matrix

trainD <- xgb.DMatrix(data = train2[,vars], label = train2[,"SalePrice"]) #Convert to xgb.DMatrix format

#Cross validate the model
# cv.sparse <- xgb.cv(data = trainD,
#                     nrounds = 1000,
#                     min_child_weight = 1,
#                     max_depth = 10,
#                     eta = 0.01,
#                     subsample = .5,
#                     colsample_bytree = .5,
#                     booster = "gbtree",
#                     eval_metric = "rmse",
#                     verbose = TRUE,
#                     print_every_n = 50,
#                     nfold = 4,
#                     nthread = 2,
#                     objective="reg:linear")

#Train the model

#Choose the parameters for the model
param <- list(colsample_bytree = .5,
             subsample = .5,
             booster = "gbtree",
             max_depth = 10,
             eta = 0.01,
             eval_metric = "rmse",
             objective="reg:linear")


#Train the model using those parameters
bstSparse <-
  xgb.train(params = param,
            data = trainD,
            nrounds = 1200,
            watchlist = list(train = trainD),
            verbose = TRUE,
            print_every_n = 50,
            nthread = 2)
```


Predict and test the RMSE.
```{r evaluate1}
testD <- xgb.DMatrix(data = test2[,vars])
#Column names must match the inputs EXACTLY
prediction <- predict(bstSparse, testD) #Make the prediction based on the half of the training data set aside

#Put testing prediction and test dataset all together
test3 <- as.data.frame(as.matrix(test2))
prediction <- as.data.frame(as.matrix(prediction))
colnames(prediction) <- "prediction"
model_output <- cbind(test3, prediction)

model_output$log_prediction <- log(model_output$prediction)
model_output$log_SalePrice <- log(model_output$SalePrice)

#Test with RMSE

rmse(model_output$log_SalePrice,model_output$log_prediction)

```

Nice, that's pretty good stuff. I'll take the xgboost I think, let's call that good and make up the submission. Honestly, this is where the interesting stuff basically ends, unless you want to see the retraining and submission formatting.

***


##Retrain on the full sample

```{r retrain}
rm(bstSparse)

#Create matrices from the data frames
retrainData<- as.matrix(train, rownames.force=NA)

#Turn the matrices into sparse matrices
retrain <- as(retrainData, "sparseMatrix")

param <- list(colsample_bytree = .7,
             subsample = .7,
             booster = "gbtree",
             max_depth = 10,
             eta = 0.02,
             eval_metric = "rmse",
             objective="reg:linear")

retrainD <- xgb.DMatrix(data = retrain[,vars], label = retrain[,"SalePrice"])

#retrain the model using those parameters
bstSparse <-
 xgb.train(params = param,
           data = retrainD,
           nrounds = 600,
           watchlist = list(train = trainD),
           verbose = TRUE,
           print_every_n = 50,
           nthread = 2)
  
```


##Prepare the prediction data

Here I just repeat the same work I did on the training set, check the code tab to see all the details.

```{r formatting_predictiondata, echo=FALSE}
test$paved[test$Street == "Pave"] <- 1
test$paved[test$Street != "Pave"] <- 0

test$regshape[test$LotShape == "Reg"] <- 1
test$regshape[test$LotShape != "Reg"] <- 0

test$flat[test$LandContour == "Lvl"] <- 1
test$flat[test$LandContour != "Lvl"] <- 0

test$pubutil[test$Utilities == "AllPub"] <- 1
test$pubutil[test$Utilities != "AllPub"] <- 0

test$gentle_slope[test$LandSlope == "Gtl"] <- 1
test$gentle_slope[test$LandSlope != "Gtl"] <- 0

test$culdesac_fr3[test$LandSlope %in% c("CulDSac", "FR3")] <- 1
test$culdesac_fr3[!test$LandSlope %in% c("CulDSac", "FR3")] <- 0

test$nbhd_price_level[test$Neighborhood %in% nbhdprice_lo$Neighborhood] <- 1
test$nbhd_price_level[test$Neighborhood %in% nbhdprice_med$Neighborhood] <- 2
test$nbhd_price_level[test$Neighborhood %in% nbhdprice_hi$Neighborhood] <- 3

test$pos_features_1[test$Condition1 %in% c("PosA", "PosN")] <- 1
test$pos_features_1[!test$Condition1 %in% c("PosA", "PosN")] <- 0

test$pos_features_2[test$Condition1 %in% c("PosA", "PosN")] <- 1
test$pos_features_2[!test$Condition1 %in% c("PosA", "PosN")] <- 0


test$twnhs_end_or_1fam[test$BldgType %in% c("1Fam", "TwnhsE")] <- 1
test$twnhs_end_or_1fam[!test$BldgType %in% c("1Fam", "TwnhsE")] <- 0

test$house_style_level[test$HouseStyle %in% housestyle_lo$HouseStyle] <- 1
test$house_style_level[test$HouseStyle %in% housestyle_med$HouseStyle] <- 2
test$house_style_level[test$HouseStyle %in% housestyle_hi$HouseStyle] <- 3


test$roof_hip_shed[test$RoofStyle %in% c("Hip", "Shed")] <- 1
test$roof_hip_shed[!test$RoofStyle %in% c("Hip", "Shed")] <- 0

test$roof_matl_hi[test$RoofMatl %in% c("Membran", "WdShake", "WdShngl")] <- 1
test$roof_matl_hi[!test$RoofMatl %in% c("Membran", "WdShake", "WdShngl")] <- 0

test$exterior_1[test$Exterior1st %in% matl_lo_1$Exterior1st] <- 1
test$exterior_1[test$Exterior1st %in% matl_med_1$Exterior1st] <- 2
test$exterior_1[test$Exterior1st %in% matl_hi_1$Exterior1st] <- 3

test$exterior_2[test$Exterior2nd %in% matl_lo$Exterior2nd] <- 1
test$exterior_2[test$Exterior2nd %in% matl_med$Exterior2nd] <- 2
test$exterior_2[test$Exterior2nd %in% matl_hi$Exterior2nd] <- 3


test$exterior_mason_1[test$MasVnrType %in% c("Stone", "BrkFace") | is.na(test$MasVnrType)] <- 1
test$exterior_mason_1[!test$MasVnrType %in% c("Stone", "BrkFace") & !is.na(test$MasVnrType)] <- 0

test$exterior_cond[test$ExterQual == "Ex"] <- 4
test$exterior_cond[test$ExterQual == "Gd"] <- 3
test$exterior_cond[test$ExterQual == "TA"] <- 2
test$exterior_cond[test$ExterQual == "Fa"] <- 1

test$exterior_cond2[test$ExterCond == "Ex"] <- 5
test$exterior_cond2[test$ExterCond == "Gd"] <- 4
test$exterior_cond2[test$ExterCond == "TA"] <- 3
test$exterior_cond2[test$ExterCond == "Fa"] <- 2
test$exterior_cond2[test$ExterCond == "Po"] <- 1


test$found_concrete[test$Foundation == "PConc"] <- 1
test$found_concrete[test$Foundation != "PConc"] <- 0


test$bsmt_cond1[test$BsmtQual == "Ex"] <- 5
test$bsmt_cond1[test$BsmtQual == "Gd"] <- 4
test$bsmt_cond1[test$BsmtQual == "TA"] <- 3
test$bsmt_cond1[test$BsmtQual == "Fa"] <- 2
test$bsmt_cond1[is.na(test$BsmtQual)] <- 1

test$bsmt_cond2[test$BsmtCond == "Gd"] <- 5
test$bsmt_cond2[test$BsmtCond == "TA"] <- 4
test$bsmt_cond2[test$BsmtCond == "Fa"] <- 3
test$bsmt_cond2[is.na(test$BsmtCond)] <- 2
test$bsmt_cond2[test$BsmtCond == "Po"] <- 1


test$bsmt_exp[test$BsmtExposure == "Gd"] <- 5
test$bsmt_exp[test$BsmtExposure == "Av"] <- 4
test$bsmt_exp[test$BsmtExposure == "Mn"] <- 3
test$bsmt_exp[test$BsmtExposure == "No"] <- 2
test$bsmt_exp[is.na(test$BsmtExposure)] <- 1


test$bsmt_fin1[test$BsmtFinType1 == "GLQ"] <- 5
test$bsmt_fin1[test$BsmtFinType1 == "Unf"] <- 4
test$bsmt_fin1[test$BsmtFinType1 == "ALQ"] <- 3
test$bsmt_fin1[test$BsmtFinType1 %in% c("BLQ", "Rec", "LwQ")] <- 2
test$bsmt_fin1[is.na(test$BsmtFinType1)] <- 1


test$bsmt_fin2[test$BsmtFinType2 == "ALQ"] <- 6
test$bsmt_fin2[test$BsmtFinType2 == "Unf"] <- 5
test$bsmt_fin2[test$BsmtFinType2 == "GLQ"] <- 4
test$bsmt_fin2[test$BsmtFinType2 %in% c("Rec", "LwQ")] <- 3
test$bsmt_fin2[test$BsmtFinType2 == "BLQ"] <- 2
test$bsmt_fin2[is.na(test$BsmtFinType2)] <- 1

test$gasheat[test$Heating %in% c("GasA", "GasW")] <- 1
test$gasheat[!test$Heating %in% c("GasA", "GasW")] <- 0

test$heatqual[test$HeatingQC == "Ex"] <- 5
test$heatqual[test$HeatingQC == "Gd"] <- 4
test$heatqual[test$HeatingQC == "TA"] <- 3
test$heatqual[test$HeatingQC == "Fa"] <- 2
test$heatqual[test$HeatingQC == "Po"] <- 1


test$air[test$CentralAir == "Y"] <- 1
test$air[test$CentralAir == "N"] <- 0

test$standard_electric[test$Electrical == "SBrkr" | is.na(test$Electrical)] <- 1
test$standard_electric[!test$Electrical == "SBrkr" & !is.na(test$Electrical)] <- 0


test$kitchen[test$KitchenQual == "Ex"] <- 4
test$kitchen[test$KitchenQual == "Gd"] <- 3
test$kitchen[test$KitchenQual == "TA"] <- 2
test$kitchen[test$KitchenQual == "Fa"] <- 1

test$fire[test$FireplaceQu == "Ex"] <- 5
test$fire[test$FireplaceQu == "Gd"] <- 4
test$fire[test$FireplaceQu == "TA"] <- 3
test$fire[test$FireplaceQu == "Fa"] <- 2
test$fire[test$FireplaceQu == "Po" | is.na(test$FireplaceQu)] <- 1


test$gar_attach[test$GarageType %in% c("Attchd", "BuiltIn")] <- 1
test$gar_attach[!test$GarageType %in% c("Attchd", "BuiltIn")] <- 0


test$gar_finish[test$GarageFinish %in% c("Fin", "RFn")] <- 1
test$gar_finish[!test$GarageFinish %in% c("Fin", "RFn")] <- 0

test$garqual[test$GarageQual == "Ex"] <- 5
test$garqual[test$GarageQual == "Gd"] <- 4
test$garqual[test$GarageQual == "TA"] <- 3
test$garqual[test$GarageQual == "Fa"] <- 2
test$garqual[test$GarageQual == "Po" | is.na(test$GarageQual)] <- 1


test$garqual2[test$GarageCond == "Ex"] <- 5
test$garqual2[test$GarageCond == "Gd"] <- 4
test$garqual2[test$GarageCond == "TA"] <- 3
test$garqual2[test$GarageCond == "Fa"] <- 2
test$garqual2[test$GarageCond == "Po" | is.na(test$GarageCond)] <- 1


test$paved_drive[test$PavedDrive == "Y"] <- 1
test$paved_drive[!test$PavedDrive != "Y"] <- 0
test$paved_drive[is.na(test$paved_drive)] <- 0

test$housefunction[test$Functional %in% c("Typ", "Mod")] <- 1
test$housefunction[!test$Functional %in% c("Typ", "Mod")] <- 0


test$pool_good[test$PoolQC %in% c("Ex")] <- 1
test$pool_good[!test$PoolQC %in% c("Ex")] <- 0

test$priv_fence[test$Fence %in% c("GdPrv")] <- 1
test$priv_fence[!test$Fence %in% c("GdPrv")] <- 0

test$sale_cat[test$SaleType %in% c("New", "Con")] <- 5
test$sale_cat[test$SaleType %in% c("CWD", "ConLI")] <- 4
test$sale_cat[test$SaleType %in% c("WD")] <- 3
test$sale_cat[test$SaleType %in% c("COD", "ConLw", "ConLD")] <- 2
test$sale_cat[test$SaleType %in% c("Oth")] <- 1

test$sale_cond[test$SaleCondition %in% c("Partial")] <- 4
test$sale_cond[test$SaleCondition %in% c("Normal", "Alloca")] <- 3
test$sale_cond[test$SaleCondition %in% c("Family","Abnorml")] <- 2
test$sale_cond[test$SaleCondition %in% c("AdjLand")] <- 1

test$zone[test$MSZoning %in% c("FV")] <- 4
test$zone[test$MSZoning %in% c("RL")] <- 3
test$zone[test$MSZoning %in% c("RH","RM")] <- 2
test$zone[test$MSZoning %in% c("C (all)")] <- 1

test$alleypave[test$Alley %in% c("Pave")] <- 1
test$alleypave[!test$Alley %in% c("Pave")] <- 0


```


```{r drop_old_vars_predictiondata, echo=FALSE}

test$Street <- NULL
test$LotShape <- NULL
test$LandContour <- NULL
test$Utilities <- NULL
test$LotConfig <- NULL
test$LandSlope <- NULL
test$Neighborhood <- NULL
test$Condition1 <- NULL
test$Condition2 <- NULL
test$BldgType <- NULL
test$HouseStyle <- NULL
test$RoofStyle <- NULL
test$RoofMatl <- NULL

test$Exterior1st <- NULL
test$Exterior2nd <- NULL
test$MasVnrType <- NULL
test$ExterQual <- NULL
test$ExterCond <- NULL

test$Foundation <- NULL
test$BsmtQual <- NULL
test$BsmtCond <- NULL
test$BsmtExposure <- NULL
test$BsmtFinType1 <- NULL
test$BsmtFinType2 <- NULL

test$Heating <- NULL
test$HeatingQC <- NULL
test$CentralAir <- NULL
test$Electrical <- NULL
test$KitchenQual <- NULL
test$FireplaceQu <- NULL

test$GarageType <- NULL
test$GarageFinish <- NULL
test$GarageQual <- NULL
test$GarageCond <- NULL
test$PavedDrive <- NULL

test$Functional <- NULL
test$PoolQC <- NULL
test$Fence <- NULL
test$MiscFeature <- NULL
test$SaleType <- NULL
test$SaleCondition <- NULL
test$MSZoning <- NULL
test$Alley <- NULL

```


```{r interactions_prediction, echo=FALSE}
#Fix some NAs

test$GarageYrBlt[is.na(test$GarageYrBlt)] <- 0
test$MasVnrArea[is.na(test$MasVnrArea)] <- 0
test$LotFrontage[is.na(test$LotFrontage)] <- 0
test$BsmtFinSF1[is.na(test$BsmtFinSF1)] <- 0
test$BsmtFinSF2[is.na(test$BsmtFinSF2)] <- 0
test$BsmtUnfSF[is.na(test$BsmtUnfSF)] <- 0
test$TotalBsmtSF[is.na(test$TotalBsmtSF)] <- 0

test$BsmtFullBath[is.na(test$BsmtFullBath)] <- 0
test$BsmtHalfBath[is.na(test$BsmtHalfBath)] <- 0
test$GarageCars[is.na(test$GarageCars)] <- 0
test$GarageArea[is.na(test$GarageArea)] <- 0
test$pubutil[is.na(test$pubutil)] <- 0


#Interactions based on correlation
test$year_qual <- test$YearBuilt*test$OverallQual #overall condition
test$year_r_qual <- test$YearRemodAdd*test$OverallQual #quality x remodel
test$qual_bsmt <- test$OverallQual*test$TotalBsmtSF #quality x basement size

test$livarea_qual <- test$OverallQual*test$GrLivArea #quality x living area
test$qual_bath <- test$OverallQual*test$FullBath #quality x baths

test$qual_ext <- test$OverallQual*test$exterior_cond #quality x exterior



```


Then, format it for xgboost, I'm just using my boilerplate code for that.
```{r finalpredict2}
# Get the supplied test data ready #

predict <- as.data.frame(test) #Get the dataset formatted as a frame for later combining

#Create matrices from the data frames
predData<- as.matrix(predict, rownames.force=NA)

#Turn the matrices into sparse matrices
predicting <- as(predData, "sparseMatrix")

```


Make sure your training sample and prediction sample have the same variables. I have been including this in code lately because I was making silly mistakes on variable choice.

```{r finalpredict3}
colnames(train[,c(2:37, 39:86)])

vars <- c("MSSubClass","LotFrontage","LotArea","OverallQual","OverallCond","YearBuilt",
 "YearRemodAdd","MasVnrArea","BsmtFinSF1","BsmtFinSF2","BsmtUnfSF","TotalBsmtSF"   ,   
 "X1stFlrSF","X2ndFlrSF","LowQualFinSF","GrLivArea","BsmtFullBath","BsmtHalfBath"  ,   
 "FullBath","HalfBath","BedroomAbvGr","KitchenAbvGr","TotRmsAbvGrd","Fireplaces"     ,  
 "GarageYrBlt","GarageCars","GarageArea","WoodDeckSF","OpenPorchSF","EnclosedPorch"    ,
 "X3SsnPorch","ScreenPorch","PoolArea","MiscVal","MoSold","YrSold",
 "paved","regshape","flat","pubutil","gentle_slope","culdesac_fr3"     ,
 "nbhd_price_level" , "pos_features_1","pos_features_2","twnhs_end_or_1fam","house_style_level", "roof_hip_shed"    ,
 "roof_matl_hi","exterior_1","exterior_2","exterior_mason_1","exterior_cond","exterior_cond2"   ,
 "found_concrete","bsmt_cond1","bsmt_cond2","bsmt_exp","bsmt_fin1","bsmt_fin2"    ,   
 "gasheat","heatqual","air","standard_electric", "kitchen","fire",
 "gar_attach","gar_finish","garqual","garqual2","paved_drive","housefunction",
 "pool_good","priv_fence","sale_cat","sale_cond","zone","alleypave",
"year_qual","year_r_qual","qual_bsmt","livarea_qual","qual_bath", "qual_ext")

#colnames(predicting)
colnames(predicting[,vars])
```

Actually do the predicting.

```{r finalpredict4}
#Column names must match the inputs EXACTLY
prediction <- predict(bstSparse, predicting[,vars])

prediction <- as.data.frame(as.matrix(prediction))  #Get the dataset formatted as a frame for later combining
colnames(prediction) <- "prediction"
model_output <- cbind(predict, prediction) #Combine the prediction output with the rest of the set

sub2 <- data.frame(Id = model_output$Id, SalePrice = model_output$prediction)
length(model_output$prediction)
write.csv(sub2, file = "sub3.csv", row.names = F)
head(sub2$SalePrice)

```